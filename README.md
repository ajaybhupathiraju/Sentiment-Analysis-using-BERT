# Sentiment-Analysis-using-BERT
BERT - Bidirectional Representation for Transformers 

BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed for Natural Language Processing (NLP) tasks.
It revolutionized the way NLP models handle text by introducing bidirectional context understanding, making it highly effective for tasks like question answering, text classification, and named entity recognition.

BERT is pre-trained on large corpora of text (e.g., Wikipedia, BookCorpus) using two tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).
we are using BERT to be fine-tuned on reviews of sentiment analysis task
